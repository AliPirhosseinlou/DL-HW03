{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbyl2XPwJJH9"
      },
      "source": [
        "\n",
        "# بخش اول − سوالات تئوری مربوط به تمرین عملی"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84bUEQBvHp50"
      },
      "source": [
        "<div dir=rtl>\n",
        "\n",
        "پاسخ :\n",
        "\n",
        "(آ) در شبکه‌های کانولوشنی معمولی، grid sampling به صورت منظم و متناوب اعمال می‌شود. این عمل به صورت یک گرید مرتب با فاصله ثابت بین نقاط اعمال می‌شود، که این نقاط به عنوان نقاط مرکزی برای اعمال عملیات کانولوشن استفاده می‌شوند.\n",
        "شبکه‌های کانولوشن Deformable با استفاده از انتقال‌دهنده‌های اختصاصی (specialized offsets) بر روی گرید ورودی، می‌توانند نقاطی را برای اعمال فیلترهای کانولوشن به دقت ترتیب دهند. این به مدل اجازه می‌دهد که نقاط مرکزی را بر اساس آموزش‌های خود تغییر دهد و به صورت فعال ، نقاطی را انتخاب کند که بهترین بازدهی را دارند.\n",
        "\n",
        "(ب)\n",
        "\n",
        "۱. آموزش Offset:\n",
        "\n",
        "ابتدا، شبکه با آموزش مکانیزم‌های Deformable ، که به عنوان offset یا جابجایی شناخته می‌شوند، می‌تواند به دقت نقاطی را برای اعمال فیلترهای کانولوشن در نظر بگیرد.\n",
        "این offset ها یادگیری می شوند و اجازه می‌دهند تا مدل به طور پویا تر و انعطاف پذیرتری به نقاط انتخاب شده برای اعمال فیلترها پاسخ دهد.\n",
        "\n",
        "۲. تطبیق Receptive Field:\n",
        "\n",
        "سپس، با استفاده از این offset ها، نقاطی که برای اعمال فیلترها در نظر گرفته می‌شوند، تغییر می‌کنند. به این ترتیب، فیلترها و نقاط مرکزی برای اعمال آنها بر اساس offset های یادگرفته شده تغییر مکان می‌دهند.\n",
        "این انتقال نقاط به مکان‌های دیگر به مدل اجازه می‌دهد تا بهتر به الگوهای نامنظم و تغییرات هندسی در تصاویر واکنش نشان دهد.\n",
        "\n",
        "(ج) در واقع در این نوع شبکه ها وقتی که تصویر دارای چرخش زیاد است آن یک تصویر جدید برای شبکه تلقی میشود که گویا مشابه آن را قبلا ندیده است و به همین علت وقتی چرخش تصویر از یک حدی بیشتر می شود با مشکل مواجه می شویم.\n",
        "\n",
        "\n",
        "(د) به وسیله فرآیند یادگیری محاسبه می شوند.این آفست‌ها برای تغییر مکان نقاط اعمال فیلترهای کانولوشن استفاده می‌شوند، به طوری که فیلترها با استفاده از این آفست‌ها، در مکان‌های مناسب تری اعمال شوند.\n",
        "در واقع این آفست ها را به عنوان پارامتر قابل یادگیری تعریف میکنیم تا بهینه ترین حالت در طی فرآیند آموزش به دست آید.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4vCJZnw6u9V"
      },
      "source": [
        "# بخش دوم-سوالات عملی"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6Kiy56OpOTr"
      },
      "source": [
        "## import libraries and data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "v9-qkoINACdc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Subset , Dataset\n",
        "from torchvision.datasets import CocoDetection\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "import torch.optim as optim\n",
        "import os\n",
        "from pycocotools.coco import COCO\n",
        "from PIL import Image\n",
        "import time\n",
        "import torchvision.ops\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoXXcdINxNZO",
        "outputId": "b9da49da-3032-46ed-a37a-e722d5557a46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ8RD2Qi7pu6"
      },
      "source": [
        "<div dir=rtl>\n",
        "\n",
        "ابتدا مطابق با دستورات زیر دیتا ست را دانلود و در گوگل درایو آنزیپ کردیم.\n",
        "\n",
        "ما فقط دیتای ولیدیشن را استفاده کردیم و آن را به دو بخش ترین و تست تقسیم خواهیم کرد."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auxpkVBJ7GKB"
      },
      "source": [
        "```python\n",
        "!wget http://images.cocodataset.org/zips/val2017.zip\n",
        "%cd /content\n",
        "!unzip val2017.zip -d /content/drive/MyDrive/DLhw3/\n",
        "\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "%cd /content\n",
        "!unzip annotations_trainval2017.zip -d /content/drive/MyDrive/DLhw3/\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEAN7gyapXlp"
      },
      "source": [
        "## define dataset class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1rHSm6ob97a"
      },
      "source": [
        "<div dir=rtl>\n",
        "در این کلاس دیتاست دلخواه خودمان را ایجاد میکنیم.\n",
        "\n",
        "به این صورت که برای هر تصویر یک لیبل به صورت یک بردار وان هات 90 تایی ایجاد میکنیم.چون 90 کلاس داریم و هر کدام که در تصویر موجود بود آن را 1 قرار می دهیم و هر کدام که موجود نبود به جای آن صفر قرار می دهیم."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "EgWC4uX24XmS"
      },
      "outputs": [],
      "source": [
        "class CustomCocoDataset(Dataset):\n",
        "    def __init__(self, root, annFile, transform=None):\n",
        "        self.root = root\n",
        "        self.coco = COCO(annFile)\n",
        "        self.transform = transform\n",
        "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        coco = self.coco\n",
        "        img_id = self.ids[index]\n",
        "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
        "        anns = coco.loadAnns(ann_ids)\n",
        "\n",
        "        path = coco.loadImgs(img_id)[0]['file_name']\n",
        "        img = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        labels = [ann['category_id'] for ann in anns]\n",
        "        labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "        labels_tensor = labels_tensor.to(device)\n",
        "        return img, labels_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwL2Ym9WXc9G",
        "outputId": "fd1a3dd5-e65c-4ae4-a155-d28b32fb8e27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=1.39s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ],
      "source": [
        "def custom_collate_fn(batch):\n",
        "    images = []\n",
        "    batch_labels = []\n",
        "\n",
        "    for img, labels in batch:\n",
        "        images.append(img)\n",
        "\n",
        "        one_hot_labels = torch.zeros(90)\n",
        "        for label in labels:\n",
        "            one_hot_labels[label-1] = 1\n",
        "\n",
        "        batch_labels.append(one_hot_labels)\n",
        "\n",
        "    images = torch.stack(images)\n",
        "    batch_labels = torch.stack(batch_labels)\n",
        "\n",
        "    return images, batch_labels\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/DLhw3/val2017'\n",
        "annotation_file = '/content/drive/MyDrive/DLhw3/annotations/instances_val2017.json'\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "coco_dataset = CustomCocoDataset(root=data_dir, annFile=annotation_file, transform=transform)\n",
        "\n",
        "train_indices = range(1000)\n",
        "test_indices = range(1000, 1300)\n",
        "\n",
        "train_dataset = Subset(coco_dataset, train_indices)\n",
        "test_dataset = Subset(coco_dataset, test_indices)\n",
        "\n",
        "batch_size=16\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrx8_tfXplxd"
      },
      "source": [
        "## model-CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "VUaBg4W8ftkR"
      },
      "outputs": [],
      "source": [
        "class CustomCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CustomCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(256 * 14 * 14, 512)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((14, 14))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = self.pool(F.relu(self.conv4(x)))\n",
        "        x = self.adaptive_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1MO9K6Spvx7"
      },
      "source": [
        "## train CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7B1zBBpfvg3",
        "outputId": "5213942e-dff0-4b4e-a737-4b002b080875"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1], Accuracy_training:72.3278%\n",
            "Epoch [1], Loss: 0.79\n",
            "Epoch [2], Accuracy_training:80.8689%\n",
            "Epoch [2], Loss: 0.79\n",
            "Epoch [3], Accuracy_training:83.0378%\n",
            "Epoch [3], Loss: 0.78\n",
            "Epoch [4], Accuracy_training:81.5289%\n",
            "Epoch [4], Loss: 0.78\n",
            "Epoch [5], Accuracy_training:80.6722%\n",
            "Epoch [5], Loss: 0.77\n",
            "Epoch [6], Accuracy_training:80.9544%\n",
            "Epoch [6], Loss: 0.77\n",
            "Epoch [7], Accuracy_training:82.8867%\n",
            "Epoch [7], Loss: 0.77\n",
            "Epoch [8], Accuracy_training:82.4667%\n",
            "Epoch [8], Loss: 0.76\n",
            "Epoch [9], Accuracy_training:83.4178%\n",
            "Epoch [9], Loss: 0.76\n",
            "Epoch [10], Accuracy_training:84.1733%\n",
            "Epoch [10], Loss: 0.75\n",
            "Epoch [11], Accuracy_training:84.3944%\n",
            "Epoch [11], Loss: 0.75\n",
            "Epoch [12], Accuracy_training:86.1122%\n",
            "Epoch [12], Loss: 0.75\n",
            "Epoch [13], Accuracy_training:86.0578%\n",
            "Epoch [13], Loss: 0.75\n",
            "Epoch [14], Accuracy_training:87.0311%\n",
            "Epoch [14], Loss: 0.74\n",
            "Epoch [15], Accuracy_training:87.4956%\n",
            "Epoch [15], Loss: 0.74\n",
            "Epoch [16], Accuracy_training:87.5311%\n",
            "Epoch [16], Loss: 0.74\n",
            "Epoch [17], Accuracy_training:88.6800%\n",
            "Epoch [17], Loss: 0.73\n",
            "Epoch [18], Accuracy_training:88.7489%\n",
            "Epoch [18], Loss: 0.74\n",
            "Epoch [19], Accuracy_training:89.0389%\n",
            "Epoch [19], Loss: 0.73\n",
            "Epoch [20], Accuracy_training:89.7111%\n",
            "Epoch [20], Loss: 0.73\n",
            "Epoch [21], Accuracy_training:90.1533%\n",
            "Epoch [21], Loss: 0.73\n",
            "Epoch [22], Accuracy_training:90.2811%\n",
            "Epoch [22], Loss: 0.73\n",
            "Epoch [23], Accuracy_training:90.4544%\n",
            "Epoch [23], Loss: 0.73\n",
            "Epoch [24], Accuracy_training:90.9133%\n",
            "Epoch [24], Loss: 0.72\n",
            "Epoch [25], Accuracy_training:91.0367%\n",
            "Epoch [25], Loss: 0.73\n",
            "Epoch [26], Accuracy_training:90.9667%\n",
            "Epoch [26], Loss: 0.72\n",
            "Epoch [27], Accuracy_training:91.3778%\n",
            "Epoch [27], Loss: 0.72\n",
            "Epoch [28], Accuracy_training:91.5711%\n",
            "Epoch [28], Loss: 0.72\n",
            "Epoch [29], Accuracy_training:91.8856%\n",
            "Epoch [29], Loss: 0.72\n",
            "Epoch [30], Accuracy_training:91.9922%\n",
            "Epoch [30], Loss: 0.72\n",
            "Total training time: 430.08 seconds\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "cnn_model = CustomCNN(num_classes=90)\n",
        "cnn_model=cnn_model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(cnn_model.parameters(), lr=0.0002)\n",
        "\n",
        "num_epochs = 30\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0\n",
        "    correct=0\n",
        "    total=0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs=inputs.to(device)\n",
        "        labels=labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = cnn_model(inputs)\n",
        "        predicted = torch.round(outputs)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    accuracy_train = 100 * correct / (total*90)\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}], Accuracy_training:{accuracy_train:.4f}%')\n",
        "    print(f'Epoch [{epoch + 1}], Loss: {running_loss / total:.2}')\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(f\"Total training time: {execution_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtCEvD8Hp07i"
      },
      "source": [
        "## test CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKv_xw-qZ1BG",
        "outputId": "843019d7-756f-481d-a4ed-b04dc7b93fc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy_test:91.6148%\n",
            "Loss: 2.4\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "test_loss=0\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(test_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs=inputs.to(device)\n",
        "        labels=labels.to(device)\n",
        "\n",
        "        outputs = cnn_model(inputs)\n",
        "        predicted = torch.round(outputs)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "    accuracy_test = 100 * correct / (total*90)\n",
        "\n",
        "    print(f'Accuracy_test:{accuracy_test:.4f}%')\n",
        "    print(f'Loss: {running_loss / total:.2}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz3r5e8qujtE"
      },
      "source": [
        "## Define DeformableConv2d class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USY4ZFH39_BR"
      },
      "source": [
        "<div dir=rtl>\n",
        "\n",
        "برای تعریف این کلاس از سورس قرار داده شده در تمرین (https://github.com/developer0hye/PyTorch-Deformable-Convolution-v2) استفاده شد.\n",
        "\n",
        "در ادامه به توضیح نحوه عملکرد این کلاس  میپردازیم.\n",
        "\n",
        "به طور خلاصه اگر بخواهیم بگوییم ،ابتدا از یک لایه کانولوشنی معمولی استفاده میکنیم برای تولید آفست ها.به این صورت که هر کدام از پیکسل ها آفست مخصوص خود را دارد.همچنین یک لایه کانولوشنی معمولی نیز به عنوان modulator قرار میدهیم که نشان دهنده شدت اعمال کانولوشن است.\n",
        "\n",
        "پس از این از ماژول کتابخانه ای torchvision.ops.deform_conv2d استفاده می شود، با استفاده از آفست ها و مدولاتور هایی که به دست آوردیم و همچنین یک لایه کانولوشنی معمولی دیگر که اعمال می شود."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "79UcF9Tpx3Y4"
      },
      "outputs": [],
      "source": [
        "class DeformableConv2d(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 kernel_size=3,\n",
        "                 stride=1,\n",
        "                 padding=1,\n",
        "                 bias=False):\n",
        "\n",
        "        super(DeformableConv2d, self).__init__()\n",
        "\n",
        "        self.padding = padding\n",
        "\n",
        "        self.offset_conv = nn.Conv2d(in_channels,\n",
        "                                     2 * kernel_size * kernel_size,\n",
        "                                     kernel_size=kernel_size,\n",
        "                                     stride=stride,\n",
        "                                     padding=self.padding,\n",
        "                                     bias=True)\n",
        "\n",
        "        nn.init.constant_(self.offset_conv.weight, 0.)\n",
        "        nn.init.constant_(self.offset_conv.bias, 0.)\n",
        "\n",
        "        self.modulator_conv = nn.Conv2d(in_channels,\n",
        "                                     1 * kernel_size * kernel_size,\n",
        "                                     kernel_size=kernel_size,\n",
        "                                     stride=stride,\n",
        "                                     padding=self.padding,\n",
        "                                     bias=True)\n",
        "\n",
        "        nn.init.constant_(self.modulator_conv.weight, 0.)\n",
        "        nn.init.constant_(self.modulator_conv.bias, 0.)\n",
        "\n",
        "        self.regular_conv = nn.Conv2d(in_channels=in_channels,\n",
        "                                      out_channels=out_channels,\n",
        "                                      kernel_size=kernel_size,\n",
        "                                      stride=stride,\n",
        "                                      padding=self.padding,\n",
        "                                      bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h, w = x.shape[2:]\n",
        "        max_offset = max(h, w)/4.\n",
        "\n",
        "        offset = self.offset_conv(x).clamp(-max_offset, max_offset)\n",
        "        modulator = 2. * torch.sigmoid(self.modulator_conv(x))\n",
        "\n",
        "        x = torchvision.ops.deform_conv2d(input=x,\n",
        "                                          offset=offset,\n",
        "                                          weight=self.regular_conv.weight,\n",
        "                                          bias=self.regular_conv.bias,\n",
        "                                          padding=self.padding,\n",
        "                                          mask=modulator\n",
        "                                          )\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7U_ZDt8Ud94"
      },
      "source": [
        "## Deformable CNN class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "OGGfOEBTvDfW"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class DeformableCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(DeformableCNN, self).__init__()\n",
        "        self.conv1 = DeformableConv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = DeformableConv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = DeformableConv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv4 = DeformableConv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(256 * 14 * 14, 512)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((14, 14))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = self.pool(F.relu(self.conv4(x)))\n",
        "        x = self.adaptive_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6kUhx7KUke5"
      },
      "source": [
        "## train Deformable CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5FC9gYFwYtL",
        "outputId": "78e7bd1f-b9b6-409c-bad7-75f96d13636e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1], Accuracy_training:75.3556%\n",
            "Epoch [1], Loss: 0.79\n",
            "Epoch [2], Accuracy_training:82.3789%\n",
            "Epoch [2], Loss: 0.79\n",
            "Epoch [3], Accuracy_training:83.7833%\n",
            "Epoch [3], Loss: 0.78\n",
            "Epoch [4], Accuracy_training:83.5711%\n",
            "Epoch [4], Loss: 0.78\n",
            "Epoch [5], Accuracy_training:83.5100%\n",
            "Epoch [5], Loss: 0.78\n",
            "Epoch [6], Accuracy_training:82.8633%\n",
            "Epoch [6], Loss: 0.77\n",
            "Epoch [7], Accuracy_training:82.1433%\n",
            "Epoch [7], Loss: 0.77\n",
            "Epoch [8], Accuracy_training:83.3133%\n",
            "Epoch [8], Loss: 0.77\n",
            "Epoch [9], Accuracy_training:84.4711%\n",
            "Epoch [9], Loss: 0.76\n",
            "Epoch [10], Accuracy_training:84.9578%\n",
            "Epoch [10], Loss: 0.76\n",
            "Epoch [11], Accuracy_training:83.5767%\n",
            "Epoch [11], Loss: 0.76\n",
            "Epoch [12], Accuracy_training:83.6522%\n",
            "Epoch [12], Loss: 0.76\n",
            "Epoch [13], Accuracy_training:85.0689%\n",
            "Epoch [13], Loss: 0.75\n",
            "Epoch [14], Accuracy_training:86.1011%\n",
            "Epoch [14], Loss: 0.75\n",
            "Epoch [15], Accuracy_training:87.1089%\n",
            "Epoch [15], Loss: 0.75\n",
            "Epoch [16], Accuracy_training:87.2000%\n",
            "Epoch [16], Loss: 0.75\n",
            "Epoch [17], Accuracy_training:86.4122%\n",
            "Epoch [17], Loss: 0.75\n",
            "Epoch [18], Accuracy_training:88.5500%\n",
            "Epoch [18], Loss: 0.74\n",
            "Epoch [19], Accuracy_training:89.2989%\n",
            "Epoch [19], Loss: 0.74\n",
            "Epoch [20], Accuracy_training:88.9989%\n",
            "Epoch [20], Loss: 0.74\n",
            "Epoch [21], Accuracy_training:89.7322%\n",
            "Epoch [21], Loss: 0.74\n",
            "Epoch [22], Accuracy_training:90.0856%\n",
            "Epoch [22], Loss: 0.74\n",
            "Epoch [23], Accuracy_training:90.0389%\n",
            "Epoch [23], Loss: 0.73\n",
            "Epoch [24], Accuracy_training:91.0800%\n",
            "Epoch [24], Loss: 0.73\n",
            "Epoch [25], Accuracy_training:91.8544%\n",
            "Epoch [25], Loss: 0.73\n",
            "Epoch [26], Accuracy_training:91.2644%\n",
            "Epoch [26], Loss: 0.73\n",
            "Epoch [27], Accuracy_training:91.5100%\n",
            "Epoch [27], Loss: 0.73\n",
            "Epoch [28], Accuracy_training:92.1133%\n",
            "Epoch [28], Loss: 0.73\n",
            "Epoch [29], Accuracy_training:92.4378%\n",
            "Epoch [29], Loss: 0.73\n",
            "Epoch [30], Accuracy_training:92.7311%\n",
            "Epoch [30], Loss: 0.73\n",
            "Total training time: 629.86 seconds\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def_cnn_model = DeformableCNN(num_classes=90)\n",
        "def_cnn_model=def_cnn_model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(def_cnn_model.parameters(), lr=0.0002)\n",
        "\n",
        "num_epochs = 30\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0\n",
        "    correct=0\n",
        "    total=0\n",
        "\n",
        "\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        inputs=inputs.to(device)\n",
        "        labels=labels.to(device)\n",
        "\n",
        "        outputs = def_cnn_model(inputs)\n",
        "        predicted = torch.round(outputs)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    accuracy_train = 100 * correct / (total*90)\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}], Accuracy_training:{accuracy_train:.4f}%')\n",
        "    print(f'Epoch [{epoch + 1}], Loss: {running_loss / total:.2}')\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(f\"Total training time: {execution_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSjNmEegHwmW",
        "outputId": "436e081d-e57b-4296-9109-2df0f3ed17c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy_test:91.5222%\n",
            "Loss: 2.4\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "test_loss=0\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(test_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs=inputs.to(device)\n",
        "        labels=labels.to(device)\n",
        "        outputs = def_cnn_model(inputs)\n",
        "        predicted = torch.round(outputs)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "    accuracy_test = 100 * correct / (total*90)\n",
        "\n",
        "    print(f'Accuracy_test:{accuracy_test:.4f}%')\n",
        "    print(f'Loss: {running_loss / total:.2}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCbcJP1ScH05"
      },
      "source": [
        "<div dir=rtl>\n",
        "\n",
        "مشاهده می شود که زمان انجام کانولوشن دفورمیبل بیشتر است و منطقی هم هست چرا که هر لایه کانولوشن دفورموبل خود شامل سه لایه کانولوشن معمولی و یکسری محاسبات دیگر و همچنین تعداد پارامترهای بیشتری نیز هست.\n",
        "\n",
        "همچنین دقت به دست آمده برای یادگیری در کانولوشن معمولی پایین تر از دفورمبل است ولی تست بالاتر از آن.\n",
        "\n",
        "لازم به ذکر است در تمام موارد دقت را برابر کل کلاس های شناسایی شده قرار دادیم.یعنی در نظر گرفتیم که از بین کلاس هایی که در بردار وان هات یک هستند کدام تشخیص داده شده است و در نهایت نیز تعداد کل را برابر تعداد ضرب در 90 در نظر گرفتیم.\n",
        "\n",
        "این موضوع به دلیل تعریف دقت بر روی داده ها است احتمالا.چون هر تصویر ممکن است شامل چند کلاس باشد و ما فقط قید بودن یا نبودن آن را در نظر گرفتیم.در جایی که اگر تعداد آنها را هم لحاظ میکردیم دقت دفورمبل تفاوت بیشتری میکرد."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "pbyl2XPwJJH9"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
